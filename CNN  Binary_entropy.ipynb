{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d625aa42-78f8-4286-be9a-823011f22256",
   "metadata": {},
   "source": [
    "# CNN Binary_Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f96b2af-1d2e-4fd2-8c9f-2e8cad495b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow library for building and training neural networks\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "315c6647-9efb-4bba-b1c7-cd31c65df041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data path https://data.caltech.edu/records/mzrjq-6wc02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fa53d4-5b69-44fa-970f-61df2ebee8e0",
   "metadata": {},
   "source": [
    "# Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79dc5768-c120-47f3-a92d-9299dff9eb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Sequential model class from Keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Import the Conv2D layer for applying 2D convolution operations\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "# Import the MaxPooling2D layer for down-sampling the input representation\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "# Import the Flatten layer to convert 2D matrix to 1D vector\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "# Import the Dense layer for adding fully connected layers\n",
    "from tensorflow.keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05a7ee36-5187-47e2-aeb4-19867516b3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sravanthi\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (256, 256, 3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6db028ad-f0a9-40a6-be03-e39fcce16537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ImageDataGenerator for real-time data augmentation and normalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create an ImageDataGenerator instance for training data with data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,         # Normalize pixel values to the range [0, 1]\n",
    "    shear_range=0.2,        # Apply random shearing transformations\n",
    "    zoom_range=0.2,         # Apply random zoom transformations\n",
    "    horizontal_flip=True    # Randomly flip images horizontally\n",
    ")\n",
    "\n",
    "# Create an ImageDataGenerator instance for test data (only normalization)\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255          # Normalize pixel values to the range [0, 1]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59dea476-d43c-47fd-8f5a-e70038dfdfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generate batches of tensor image data with real-time data augmentation\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    \"C:\\\\Users\\\\sravanthi\\\\OneDrive\\\\Desktop\\\\CNN_train\",  # Directory path where training images are stored\n",
    "    target_size=(256, 256),  # Resize all images to 256x256 pixels\n",
    "    batch_size=32,           # Number of images to return in each batch\n",
    "    class_mode='binary'      # Type of label encoding (binary classification)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6c3fff3-1760-475c-9473-97bad4807dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generate batches of tensor image data for testing with normalization only\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    \"C:\\\\Users\\\\sravanthi\\\\OneDrive\\\\Desktop\\\\CNN_test\",  # Directory path where test images are stored\n",
    "    target_size=(256, 256),  # Resize all images to 256x256 pixels\n",
    "    batch_size=32,           # Number of images to return in each batch\n",
    "    class_mode='binary'      # Type of label encoding (binary classification)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab83bad5-8a48-4c95-884d-2cfa40cb3c7a",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5db8a4ef-06fd-414c-bad1-ed0e034f4e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sravanthi\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/80\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:47\u001b[0m 3s/step - accuracy: 0.6000 - loss: 0.6809"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sravanthi\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6000 - loss: 0.6809 - val_accuracy: 0.5263 - val_loss: 1.7869\n",
      "Epoch 2/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5500 - loss: 1.6061 - val_accuracy: 0.4737 - val_loss: 11.8628\n",
      "Epoch 3/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5000 - loss: 10.3445 - val_accuracy: 0.4737 - val_loss: 7.3142\n",
      "Epoch 4/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5000 - loss: 6.3033 - val_accuracy: 0.4737 - val_loss: 1.1532\n",
      "Epoch 5/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5000 - loss: 0.9433 - val_accuracy: 0.5263 - val_loss: 2.8035\n",
      "Epoch 6/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5000 - loss: 2.7314 - val_accuracy: 0.5263 - val_loss: 3.3599\n",
      "Epoch 7/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5000 - loss: 3.3822 - val_accuracy: 0.5263 - val_loss: 2.5097\n",
      "Epoch 8/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5000 - loss: 2.5469 - val_accuracy: 0.5263 - val_loss: 1.4300\n",
      "Epoch 9/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5000 - loss: 1.3859 - val_accuracy: 0.5263 - val_loss: 0.8167\n",
      "Epoch 10/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5000 - loss: 0.7585 - val_accuracy: 0.5263 - val_loss: 0.6097\n",
      "Epoch 11/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6000 - loss: 0.5418 - val_accuracy: 0.6842 - val_loss: 0.6014\n",
      "Epoch 12/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9000 - loss: 0.4836 - val_accuracy: 0.6316 - val_loss: 0.6707\n",
      "Epoch 13/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7000 - loss: 0.5517 - val_accuracy: 0.6316 - val_loss: 0.6684\n",
      "Epoch 14/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7500 - loss: 0.5106 - val_accuracy: 0.6842 - val_loss: 0.6009\n",
      "Epoch 15/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8500 - loss: 0.4671 - val_accuracy: 0.6842 - val_loss: 0.5531\n",
      "Epoch 16/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9000 - loss: 0.4166 - val_accuracy: 0.5789 - val_loss: 0.6008\n",
      "Epoch 17/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6500 - loss: 0.4523 - val_accuracy: 0.6316 - val_loss: 0.5749\n",
      "Epoch 18/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7500 - loss: 0.4000 - val_accuracy: 0.7368 - val_loss: 0.4749\n",
      "Epoch 19/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.2309 - val_accuracy: 0.6842 - val_loss: 0.7200\n",
      "Epoch 20/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8500 - loss: 0.2943 - val_accuracy: 0.7368 - val_loss: 0.5460\n",
      "Epoch 21/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.1231 - val_accuracy: 0.6842 - val_loss: 0.6337\n",
      "Epoch 22/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0849 - val_accuracy: 0.6842 - val_loss: 0.7266\n",
      "Epoch 23/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0646 - val_accuracy: 0.7895 - val_loss: 0.8402\n",
      "Epoch 24/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9500 - loss: 0.1403 - val_accuracy: 0.6842 - val_loss: 1.2139\n",
      "Epoch 25/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9500 - loss: 0.1268 - val_accuracy: 0.7895 - val_loss: 0.9291\n",
      "Epoch 26/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0207 - val_accuracy: 0.6842 - val_loss: 0.8591\n",
      "Epoch 27/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0584 - val_accuracy: 0.6842 - val_loss: 0.9067\n",
      "Epoch 28/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0439 - val_accuracy: 0.7368 - val_loss: 0.9293\n",
      "Epoch 29/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0194 - val_accuracy: 0.7895 - val_loss: 1.0242\n",
      "Epoch 30/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0113 - val_accuracy: 0.7895 - val_loss: 1.1288\n",
      "Epoch 31/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.7368 - val_loss: 1.2708\n",
      "Epoch 32/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0273 - val_accuracy: 0.7895 - val_loss: 1.0724\n",
      "Epoch 33/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.7368 - val_loss: 0.9687\n",
      "Epoch 34/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.6842 - val_loss: 0.9337\n",
      "Epoch 35/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.7368 - val_loss: 0.9327\n",
      "Epoch 36/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.7368 - val_loss: 0.9455\n",
      "Epoch 37/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0065 - val_accuracy: 0.7368 - val_loss: 0.9618\n",
      "Epoch 38/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0041 - val_accuracy: 0.7368 - val_loss: 0.9768\n",
      "Epoch 39/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.6842 - val_loss: 0.9954\n",
      "Epoch 40/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.6842 - val_loss: 1.0160\n",
      "Epoch 41/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.6842 - val_loss: 1.0387\n",
      "Epoch 42/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0042 - val_accuracy: 0.6842 - val_loss: 1.0593\n",
      "Epoch 43/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.6842 - val_loss: 1.0813\n",
      "Epoch 44/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0192 - val_accuracy: 0.6842 - val_loss: 1.0526\n",
      "Epoch 45/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.6842 - val_loss: 1.0358\n",
      "Epoch 46/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.6842 - val_loss: 1.0263\n",
      "Epoch 47/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 0.7368 - val_loss: 1.0201\n",
      "Epoch 48/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.7368 - val_loss: 1.0158\n",
      "Epoch 49/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.7368 - val_loss: 1.0130\n",
      "Epoch 50/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 0.7368 - val_loss: 1.0075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x17923821e20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model using the training dataset and validate using the test dataset\n",
    "classifier.fit(\n",
    "    training_set,              # Training data generator that yields batches of augmented images\n",
    "    steps_per_epoch=80,        # Number of steps (batches of samples) to draw from the training set for each epoch\n",
    "    epochs=50,                 # Number of epochs (complete passes through the training dataset) to train the model\n",
    "    validation_data=test_set,  # Validation data generator for evaluating the model's performance after each epoch\n",
    "    validation_steps=80        # Number of steps (batches of samples) to draw from the validation set for each epoch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1fc9b7-8292-484e-9d7f-7de21fed4be8",
   "metadata": {},
   "source": [
    "# Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0afff5d4-eeb0-46f6-a55d-8cdcab9eebdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "# Load and preprocess the image for prediction\n",
    "test_image = image.load_img(\n",
    "    \"C:\\\\Users\\\\sravanthi\\\\OneDrive\\\\Desktop\\\\CNN_Bin\\\\butterfly\\\\image_0003.jpg\",  # Path to the image file\n",
    "    target_size=(256, 256)  # Resize the image to match the input size expected by the model\n",
    ")\n",
    "\n",
    "# Convert the image to a numpy array\n",
    "test_image = image.img_to_array(test_image)\n",
    "\n",
    "# Expand dimensions to match the input shape of the model (batch size of 1)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "# Predict the class probabilities of the image\n",
    "result = classifier.predict(test_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56061abf-ef38-4131-bf95-71244cb92209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30aacd67-0064-4a3b-a030-806b6e5a2e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "# Load and preprocess the image for prediction\n",
    "test_image = image.load_img(\n",
    "    \"C:\\\\Users\\\\sravanthi\\\\OneDrive\\\\Desktop\\\\CNN_Bin\\\\cups\\\\image_0006.jpg\",  # Path to the image file\n",
    "    target_size=(256, 256)  # Resize the image to the target size expected by the model\n",
    ")\n",
    "\n",
    "# Convert the loaded image to a numpy array\n",
    "test_image = image.img_to_array(test_image)\n",
    "\n",
    "# Expand dimensions to add a batch dimension (the model expects a batch of images)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "# Predict the class probabilities for the given image\n",
    "result = classifier.predict(test_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3974e5ff-a483-4fbf-8e82-7c7dd271c1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafbe8c5-c002-4143-9ac7-4e6a7deb9536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
